{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from multigraph import MultiGraph\n",
    "\n",
    "with open('adj_graph.pkl', 'rb') as inobj:\n",
    "    op_graph = pickle.load(inobj)\n",
    "\n",
    "a_graph = op_graph.get_adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: GraphConvolution",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff253056d670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rgcn_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mmodel_from_json\u001b[0;34m(json_string, custom_objects)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/layers/__init__.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 1003\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/layers/__init__.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: GraphConvolution"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Activation\n",
    "\n",
    "from rgcn.utils import *\n",
    "from rgcn.layers.graph import GraphConvolution\n",
    "from rgcn.layers.input_adj import InputAdj\n",
    "\n",
    "#load the saved embeddings\n",
    "# load json and create model\n",
    "json_file = open('rgcn_model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"rgcn_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "# Create list of adjacency matrices, transpose, and identity\n",
    "A = list()\n",
    "for i in range(num_relations):\n",
    "    k = op_graph.get_relation_label(i)\n",
    "    A.append(op_graph.get_adjacency_matrix_k(k))\n",
    "    A.append(op_graph.get_transpose_adjacency_matrix_k(k))\n",
    "A.append(sp.identity(num_nodes).tocsr())\n",
    "support = len(A)\n",
    "\n",
    "# Normalize adjacency matrices\n",
    "for i in range(len(A)):\n",
    "    d = np.array(A[i].sum(1)).flatten()\n",
    "    d_inv = 1. / d\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    D_inv = sp.diags(d_inv)\n",
    "    A[i] = D_inv.dot(A[i]).tocsr()\n",
    "    \n",
    "X = sp.csr_matrix(A[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.predict([X] + A, batch_size=num_nodes)\n",
    "#embedding = model.predict(a_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each node in the AIFB dataset is either a person, a project, a publication, a group, etc.\n",
    "# Here we get a list of all of the nodes that have a certain label and put them in a dictionary\n",
    "import re\n",
    "publications = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Publikationen\\/viewPublikationOWL\\/id[0-9]*instance)>\")\n",
    "authors = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Personen\\/viewPersonOWL\\/id[0-9]*instance)>\")\n",
    "groups = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Forschungsgruppen\\/viewForschungsgruppeOWL\\/id[0-9]*instance)>\")\n",
    "projects = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Projekte\\/viewProjektOWL\\/id[0-9]*instance)>\")\n",
    "subjects = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Forschungsgebiete\\/viewForschungsgebietOWL\\/id[0-9]*instance)>\")\n",
    "organizations = re.compile(\"<(http:\\/\\/www\\.aifb\\.uni-karlsruhe\\.de\\/Kooperationen\\/viewOrganizationOWL\\/id[0-9]*instance)>\")\n",
    "\n",
    "regex_dict = {\"publications\":publications, \"authors\":authors, \n",
    "              \"groups\":groups, \"projects\": projects, \"subjects\": subjects,\n",
    "              \"organizations\":organizations}\n",
    "\n",
    "from collections import defaultdict\n",
    "categories = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"aifb_csv/aifb_stripped.nt\") as f:\n",
    "    for line in f:\n",
    "        for k,reg in regex_dict.iteritems():\n",
    "            finds = reg.match(line)\n",
    "            if finds != None:\n",
    "                categories[k].add(finds.group(1))\n",
    "\n",
    "for k,v in categories.iteritems():\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#get the embeddings for 100 instances from each category\n",
    "subset_embeddings = defaultdict(list)\n",
    "label_enum = list(enumerate(categories.keys(),1))\n",
    "label_dict = {j: i for i,j in label_enum}\n",
    "\n",
    "for k,v in categories.iteritems():\n",
    "    samples = random.sample(v, min(len(v), 100))\n",
    "    for s in samples:\n",
    "        #print(s,embedding[op_graph.nodes[s]])\n",
    "        subset_embeddings[k].append(embedding[op_graph.nodes[s]])\n",
    "print(label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import gensim\n",
    "\n",
    "\n",
    "#from https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "def tsnescatterplot(vector_dict, labels):\n",
    "    #flatten the dictionary into two lists\n",
    "    vectors = []\n",
    "    vector_labels = []\n",
    "    for k,v in vector_dict.iteritems():\n",
    "        i = [l[0] for l in labels if l[1] == k]\n",
    "        for s in v:\n",
    "            vectors.append(s)\n",
    "            vector_labels.append(i[0])\n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    # display scatter plot   \n",
    "    cmap = plt.cm.rainbow\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=len(labels))\n",
    "    plt.figure(figsize=(8,6),dpi=200)\n",
    "    plt.scatter(x_coords, y_coords, alpha=0.85, c=[cmap(norm(i)) for i in vector_labels])\n",
    "    #draw the legend\n",
    "    patches = []\n",
    "    for l in labels:\n",
    "        if l[1] in vector_dict:\n",
    "            patches.append(mpatches.Patch(color=cmap(norm(l[0])), label=l[1]))\n",
    "    plt.legend(handles=patches)\n",
    "    #scatterpoints=1, loc='lower left', ncol=3, fontsize=8\n",
    "\n",
    "    #for label, x, y in zip(vector_labels, x_coords, y_coords):\n",
    "    #    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-5.5, x_coords.max()+5.5)\n",
    "    plt.ylim(y_coords.min()-5.5, y_coords.max()+5.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def umapscatterplot(vector_dict, labels):\n",
    "    #flatten the dictionary into two lists\n",
    "    vectors = []\n",
    "    vector_labels = []\n",
    "    for k,v in vector_dict.iteritems():\n",
    "        i = [l[0] for l in labels if l[1] == k]\n",
    "        for s in v:\n",
    "            vectors.append(s)\n",
    "            vector_labels.append(i[0])\n",
    "    # find tsne coords for 2 dimensions\n",
    "    umapper = umap.UMAP()\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = umapper.fit_transform(vectors)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    # display scatter plot   \n",
    "    cmap = plt.cm.rainbow\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=len(labels))\n",
    "    plt.figure(figsize=(8,6),dpi=200)\n",
    "    plt.scatter(x_coords, y_coords, alpha=0.85, c=[cmap(norm(i)) for i in vector_labels])\n",
    "    #draw the legend\n",
    "    patches = []\n",
    "    for l in labels:\n",
    "        if l[1] in vector_dict:\n",
    "            patches.append(mpatches.Patch(color=cmap(norm(l[0])), label=l[1]))\n",
    "    plt.legend(handles=patches)\n",
    "    #scatterpoints=1, loc='lower left', ncol=3, fontsize=8\n",
    "\n",
    "    #for label, x, y in zip(vector_labels, x_coords, y_coords):\n",
    "    #    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-5.5, x_coords.max()+5.5)\n",
    "    plt.ylim(y_coords.min()-5.5, y_coords.max()+5.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pcascatterplot(vector_dict, labels):\n",
    "    #flatten the dictionary into two lists\n",
    "    vectors = []\n",
    "    vector_labels = []\n",
    "    for k,v in vector_dict.iteritems():\n",
    "        i = [l[0] for l in labels if l[1] == k]\n",
    "        for s in v:\n",
    "            vectors.append(s)\n",
    "            vector_labels.append(i[0])\n",
    "    # find tsne coords for 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = pca.fit_transform(vectors)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    cmap = plt.cm.rainbow\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=len(labels))\n",
    "    # display scatter plot\n",
    "    plt.figure(figsize=(8,6),dpi=200)\n",
    "    plt.scatter(x_coords, y_coords,alpha=0.5, c=[cmap(norm(i)) for i in vector_labels])\n",
    "    \n",
    "   #draw the legend\n",
    "    patches = []\n",
    "    for l in labels:\n",
    "        if l[1] in vector_dict:\n",
    "            patches.append(mpatches.Patch(color=cmap(norm(l[0])), label=l[1]))\n",
    "    plt.legend(handles=patches)\n",
    "    #scatterpoints=1, loc='lower left', ncol=3, fontsize=8\n",
    "    \n",
    "    #for label, x, y in zip(vector_labels, x_coords, y_coords):\n",
    "    #    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-5.5, x_coords.max()+5.5)\n",
    "    plt.ylim(y_coords.min()-5.5, y_coords.max()+5.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapscatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "def get_class_embeddings(full_embeddings, graph, filename):\n",
    "    sub_dict = defaultdict(list)\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        graphreader = csv.reader(csvfile)\n",
    "        i = 0\n",
    "        for row in graphreader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            sub_dict[row[1].rsplit('/',2)[-1]].append(row[0])\n",
    "\n",
    "    print(\"Number of instances in each label:\")\n",
    "    for k,v in sub_dict.iteritems():\n",
    "        print(k,len(v))\n",
    "    #get the embeddings for each of the classes in the sub_dict\n",
    "    subset_embeddings = defaultdict(list)\n",
    "    #subset_labels = []\n",
    "    label_enum = list(enumerate(sub_dict.keys(),1))\n",
    "    #label_dict = {j: i for i,j in label_enum}\n",
    "\n",
    "    for k,v in sub_dict.iteritems():\n",
    "        for s in v:\n",
    "            subset_embeddings[k].append(full_embeddings[graph.nodes[s]])\n",
    "            #subset_labels.append(label_dict[k])\n",
    "    print(\"enumerating the labels:\")\n",
    "    print(label_enum)\n",
    "    return (subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AIFB dataset has 176 people and they are associated with one of four affiliations in the organization.\n",
    "# The completeSet.tsv file gives the URIs of the people and their affiliation\n",
    "# (number, person, affiliation)\n",
    "\n",
    "subset_embeddings, label_enum = get_class_embeddings(embedding,op_graph,'aifb_csv/completeDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapscatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_embeddings, label_enum = get_class_embeddings(embedding,op_graph,'aifb_csv/project_carried_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapscatterplot(subset_embeddings, label_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_embeddings, finance_enum = get_class_embeddings(embedding,op_graph,'aifb_csv/projects_financed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the financial sources that financed more than 3 projects\n",
    "finance_embeddings = {k: v for k, v in finance_embeddings.iteritems() if len(v) > 3}\n",
    "print(\"number of instances in each label:\")\n",
    "for k,v in finance_embeddings.iteritems():\n",
    "    print (k,len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(finance_embeddings, finance_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapscatterplot(finance_embeddings, finance_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
