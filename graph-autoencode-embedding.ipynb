{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multigraph import MultiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading relation 0...\n",
      "loading relation 1...\n",
      "loading relation 2...\n",
      "loading relation 3...\n",
      "loading relation 4...\n",
      "loading relation 5...\n",
      "loading relation 6...\n",
      "loading relation 7...\n",
      "loading relation 8...\n",
      "loading relation 9...\n",
      "loading relation 10...\n",
      "loading relation 11...\n",
      "loading relation 12...\n",
      "loading relation 13...\n",
      "loading relation 14...\n",
      "loading relation 15...\n",
      "loading relation 16...\n",
      "loading relation 17...\n",
      "loading relation 18...\n",
      "loading relation 19...\n",
      "loading relation 20...\n",
      "loading relation 21...\n",
      "loading relation 22...\n",
      "loading relation 23...\n",
      "loading relation 24...\n",
      "loading relation 25...\n",
      "loading relation 26...\n",
      "loading relation 27...\n",
      "loading relation 28...\n",
      "loading relation 29...\n",
      "loading relation 30...\n",
      "loading relation 31...\n",
      "loading relation 32...\n",
      "loading relation 33...\n",
      "loading relation 34...\n",
      "loading relation 35...\n",
      "loading relation 36...\n",
      "loading relation 37...\n",
      "loading relation 38...\n",
      "loading relation 39...\n",
      "loading relation 40...\n",
      "loading relation 41...\n",
      "loading relation 42...\n",
      "loading relation 43...\n",
      "loading relation 44...\n"
     ]
    }
   ],
   "source": [
    "#import csv\n",
    "import unicodecsv as csv\n",
    "\"\"\" EXAMPLE\n",
    "add,IS,operator\n",
    "subtract,IS,operator\n",
    "multiply,IS,operator\n",
    "divide,IS,operator\n",
    "open_closure,IS,operator\n",
    "close_closure,IS,operator\n",
    "\"\"\"\n",
    "op_graph = MultiGraph()\n",
    "#with open('operator_graph.csv', 'r') as csvfile:\n",
    "for i in range(45):\n",
    "    print('loading relation '+str(i)+'...')\n",
    "    with open('aifb_csv/aifb_relation_'+str(i)+'.csv', 'r') as csvfile:\n",
    "        graphreader = csv.reader(csvfile, delimiter=\",\")\n",
    "        for row in graphreader:\n",
    "            #print(row)\n",
    "            op_graph.add_connection(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of nodes', 8284)\n",
      "('number of relations', 45)\n",
      "relations:\n",
      "(u'ontology#number', 145)\n",
      "(u'ontology#isWorkedOnBy', 571)\n",
      "(u'ontology#worksAtProject', 200)\n",
      "(u'owl#allValuesFrom', 152)\n",
      "(u'ontology#dealtWithIn', 357)\n",
      "(u'owl#onProperty', 152)\n",
      "(u'ontology#type', 50)\n",
      "(u'ontology#author', 3986)\n",
      "(u'ontology#abstract', 534)\n",
      "(u'ontology#carriedOutBy', 79)\n",
      "(u'ontology#month', 759)\n",
      "(u'ontology#phone', 227)\n",
      "(u'22-rdf-syntax-ns#type', 4124)\n",
      "(u'ontology#address', 202)\n",
      "(u'ontology#note', 114)\n",
      "(u'ontology#publication', 4163)\n",
      "(u'ontology#financedBy', 65)\n",
      "(u'ontology#chapter', 15)\n",
      "(u'ontology#editor', 190)\n",
      "(u'ontology#pages', 548)\n",
      "(u'owl#inverseOf', 10)\n",
      "(u'ontology#projectInfo', 952)\n",
      "(u'type', 129)\n",
      "(u'ontology#edition', 12)\n",
      "(u'ontology#booktitle', 765)\n",
      "(u'ontology#isAbout', 2477)\n",
      "(u'ontology#finances', 68)\n",
      "(u'ontology#howpublished', 49)\n",
      "(u'ontology#member', 339)\n",
      "(u'ontology#hasProject', 952)\n",
      "(u'ontology#isbn', 16)\n",
      "(u'ontology#journal', 161)\n",
      "(u'ontology#year', 1227)\n",
      "(u'ontology#title', 1227)\n",
      "(u'ontology#fax', 227)\n",
      "(u'rdf-schema#range', 1)\n",
      "(u'ontology#homepage', 239)\n",
      "(u'ontology#carriesOut', 79)\n",
      "(u'ontology#photo', 148)\n",
      "(u'rdf-schema#subClassOf', 199)\n",
      "(u'ontology#head', 5)\n",
      "(u'ontology#volume', 311)\n",
      "(u'ontology#publishes', 1217)\n",
      "(u'ontology#name', 1302)\n",
      "(u'ontology#series', 298)\n"
     ]
    }
   ],
   "source": [
    "a_graph = op_graph.get_adjacency_matrix()\n",
    "print(\"number of nodes\",op_graph.n_nodes)\n",
    "print(\"number of relations\",op_graph.n_rels)\n",
    "print(\"relations:\")\n",
    "for k,v in op_graph.rel_counter.iteritems():\n",
    "    print(k.rsplit('/',1)[-1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input dims:', (8284, 372780))\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x372780 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = a_graph\n",
    "print('input dims:',x_train.shape)\n",
    "print(type(x_train))\n",
    "sum(list(x_train[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make a custom Dense layer to tie weights between the encoder and decoder\n",
    "from keras import backend as K\n",
    "\n",
    "class DenseTied(Dense):\n",
    "    def __init__(self, output_dim, master_layer, **kwargs):\n",
    "        #output_dim needs to be equal to the input dimensions of the master_layer\n",
    "        self.master_layer = master_layer\n",
    "        self.output_dim = output_dim\n",
    "        super(DenseTied, self).__init__(self.output_dim, **kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.kernel = K.transpose(self.master_layer.kernel)\n",
    "        super(DenseTied, self).build(input_shape)\n",
    "        \n",
    "    #def call(self, inputs):\n",
    "    #    output = K.dot(inputs, K.transpose(self.master_layer.kernel))\n",
    "    #    output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "    #    if self.activation is not None:\n",
    "    #        output = self.activation(output)\n",
    "    #    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 372780)            0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 512)               190863872 \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "coding_layer (Dense)         (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "decode_layer1 (DenseTied)    (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "decode_layer2 (DenseTied)    (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "reconstruction_layer (Dense) (None, 372780)            191236140 \n",
      "=================================================================\n",
      "Total params: 382,428,844\n",
      "Trainable params: 382,428,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 128\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(input_dim,),name=\"inputs\")\n",
    "# Encoder Layers\n",
    "encode_layer1 = Dense(4 * encoding_dim, activation='tanh',name=\"encode_layer1\")\n",
    "encode_layer2 = Dense(2 * encoding_dim, activation='tanh',name=\"encode_layer2\")\n",
    "coding_layer = Dense(encoding_dim, activation='tanh',name=\"coding_layer\")\n",
    "\n",
    "encoding_1 = encode_layer1(inputs)\n",
    "encoding_2 = encode_layer2(encoding_1)\n",
    "the_code = coding_layer(encoding_2)\n",
    "\n",
    "# Decoder Layers\n",
    "#decode_layer1 = Dense(2 * encoding_dim, activation='tanh',name=\"decode_layer1\")\n",
    "#decode_layer2 = Dense(4 * encoding_dim, activation='tanh',name=\"decode_layer2\")\n",
    "#reconstruction_layer = Dense(input_dim, activation='tanh',name=\"reconstruction_layer\")\n",
    "decode_layer1 = DenseTied(2 * encoding_dim, coding_layer, activation='tanh',name=\"decode_layer1\")\n",
    "decode_layer2 = DenseTied(4 * encoding_dim, encode_layer2, activation='tanh',name=\"decode_layer2\")\n",
    "recon_layer = DenseTied(input_dim, encode_layer1, activation='tanh',name=\"reconstruction_layer\")\n",
    "\n",
    "decoding_1 = decode_layer1(the_code)\n",
    "decoding_2 = decode_layer2(decoding_1)\n",
    "reconstruction = reconstruction_layer(decoding_2)\n",
    "\n",
    "ae = Model(inputs=inputs, outputs=reconstruction)\n",
    "#monitor = EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 352/8284 [>.............................] - ETA: 16:36 - loss: 8.1241e-06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-afe5703a0016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/embeddings/lib/python2.7/site-packages/Theano-1.0.3-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae.fit(x_train, x_train.toarray(), epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding_model = Model(inputs=ae.inputs, outputs=ae.get_layer(\"the_code\").output)\n",
    "coding_model = Model(inputs=inputs, outputs=the_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the embeddings in order to plot them later\n",
    "# serialize model to JSON\n",
    "model_json = coding_model.to_json()\n",
    "with open(\"coding_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "coding_model.save_weights(\"coding_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#save the adjacency matrix as well\n",
    "save_object(op_graph, 'adj_graph.pkl')\n",
    "print(\"Saved graph object to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
