{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multigraph import MultiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "import unicodecsv as csv\n",
    "\"\"\" EXAMPLE\n",
    "add,IS,operator\n",
    "subtract,IS,operator\n",
    "multiply,IS,operator\n",
    "divide,IS,operator\n",
    "open_closure,IS,operator\n",
    "close_closure,IS,operator\n",
    "\"\"\"\n",
    "op_graph = MultiGraph()\n",
    "#with open('operator_graph.csv', 'r') as csvfile:\n",
    "n = 45\n",
    "for i in range(n):\n",
    "    print '.',\n",
    "    with open('aifb_csv/aifb_relation_'+str(i)+'.csv', 'r') as csvfile:\n",
    "        graphreader = csv.reader(csvfile, delimiter=\",\")\n",
    "        for row in graphreader:\n",
    "            op_graph.add_connection(row)\n",
    "print('\\n loaded '+str(op_graph.n_rels)+' relations.')\n",
    "\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#save the adjacency matrix as well\n",
    "save_object(op_graph, 'adj_graph.pkl')\n",
    "print(\"Saved graph object to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('adj_graph.pkl', 'rb') as inobj:\n",
    "    op_graph = pickle.load(inobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of nodes', 8284)\n",
      "('number of relations', 45)\n",
      "relations:\n",
      "(u'ontology#publishes', 1217)\n",
      "(u'ontology#number', 145)\n",
      "(u'ontology#isWorkedOnBy', 571)\n",
      "(u'ontology#worksAtProject', 200)\n",
      "(u'ontology#dealtWithIn', 357)\n",
      "(u'owl#onProperty', 152)\n",
      "(u'ontology#type', 50)\n",
      "(u'ontology#author', 3986)\n",
      "(u'ontology#abstract', 534)\n",
      "(u'ontology#carriedOutBy', 79)\n",
      "(u'ontology#month', 759)\n",
      "(u'ontology#phone', 227)\n",
      "(u'22-rdf-syntax-ns#type', 4124)\n",
      "(u'ontology#address', 202)\n",
      "(u'ontology#note', 114)\n",
      "(u'ontology#publication', 4163)\n",
      "(u'ontology#financedBy', 65)\n",
      "(u'ontology#editor', 190)\n",
      "(u'ontology#pages', 548)\n",
      "(u'owl#inverseOf', 10)\n",
      "(u'ontology#projectInfo', 952)\n",
      "(u'rdf-schema#range', 1)\n",
      "(u'ontology#booktitle', 765)\n",
      "(u'ontology#isAbout', 2477)\n",
      "(u'ontology#finances', 68)\n",
      "(u'ontology#howpublished', 49)\n",
      "(u'ontology#member', 339)\n",
      "(u'owl#allValuesFrom', 152)\n",
      "(u'ontology#edition', 12)\n",
      "(u'ontology#isbn', 16)\n",
      "(u'ontology#hasProject', 952)\n",
      "(u'ontology#journal', 161)\n",
      "(u'ontology#year', 1227)\n",
      "(u'ontology#title', 1227)\n",
      "(u'ontology#fax', 227)\n",
      "(u'ontology#homepage', 239)\n",
      "(u'ontology#name', 1302)\n",
      "(u'ontology#carriesOut', 79)\n",
      "(u'ontology#photo', 148)\n",
      "(u'rdf-schema#subClassOf', 199)\n",
      "(u'ontology#head', 5)\n",
      "(u'ontology#volume', 311)\n",
      "(u'ontology#chapter', 15)\n",
      "(u'type', 129)\n",
      "(u'ontology#series', 298)\n"
     ]
    }
   ],
   "source": [
    "a_graph = op_graph.get_adjacency_matrix()\n",
    "print(\"number of nodes\",op_graph.n_nodes)\n",
    "print(\"number of relations\",op_graph.n_rels)\n",
    "print(\"relations:\")\n",
    "for k,v in op_graph.rel_counter.iteritems():\n",
    "    print(k.rsplit('/',1)[-1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input dims:', (8284, 372780))\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x372780 sparse matrix of type '<type 'numpy.int8'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = a_graph\n",
    "print('input dims:',x_train.shape)\n",
    "print(type(x_train))\n",
    "sum(list(x_train[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make a custom Dense layer to tie weights between the encoder and decoder\n",
    "from keras import backend as K\n",
    "\n",
    "class DenseTied(Dense):\n",
    "    def __init__(self, master_layer, **kwargs):\n",
    "        #output_dim needs to be equal to the input dimensions of the master_layer\n",
    "        self.output_dim = master_layer.input_shape[-1]\n",
    "        super(DenseTied, self).__init__(self.output_dim, **kwargs)\n",
    "        self.master_layer = master_layer\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = K.transpose(self.master_layer.kernel)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
    "                            initializer=self.bias_initializer,\n",
    "                            name='bias',\n",
    "                            regularizer=self.bias_regularizer,\n",
    "                            constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, K.transpose(self.master_layer.kernel))\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 372780)            0         \n",
      "_________________________________________________________________\n",
      "encode_layer1 (Dense)        (None, 64)                23857984  \n",
      "_________________________________________________________________\n",
      "encode_layer2 (Dense)        (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "coding_layer (Dense)         (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "decode_layer1 (DenseTied)    (None, 32)                32        \n",
      "_________________________________________________________________\n",
      "decode_layer2 (DenseTied)    (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "reconstruction_layer (DenseT (None, 372780)            372780    \n",
      "=================================================================\n",
      "Total params: 24,233,468\n",
      "Trainable params: 24,233,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 16\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(input_dim,), sparse=True, name=\"inputs\")\n",
    "# Encoder Layers\n",
    "encode_layer1 = Dense(4 * encoding_dim, activation='relu', name=\"encode_layer1\")\n",
    "encode_layer2 = Dense(2 * encoding_dim, activation='relu', name=\"encode_layer2\")\n",
    "coding_layer = Dense(encoding_dim, activation='relu', name=\"coding_layer\")\n",
    "\n",
    "encoding_1 = encode_layer1(inputs)\n",
    "encoding_2 = encode_layer2(encoding_1)\n",
    "the_code = coding_layer(encoding_2)\n",
    "\n",
    "# Decoder Layers\n",
    "#decode_layer1 = Dense(2 * encoding_dim, activation='tanh',name=\"decode_layer1\")\n",
    "#decode_layer2 = Dense(4 * encoding_dim, activation='tanh',name=\"decode_layer2\")\n",
    "#reconstruction_layer = Dense(input_dim, activation='tanh',name=\"reconstruction_layer\")\n",
    "decode_layer1 = DenseTied(coding_layer, activation='relu',name=\"decode_layer1\")\n",
    "decode_layer2 = DenseTied(encode_layer2, activation='relu',name=\"decode_layer2\")\n",
    "recon_layer = DenseTied(encode_layer1, activation='relu',name=\"reconstruction_layer\")\n",
    "\n",
    "decoding_1 = decode_layer1(the_code)\n",
    "decoding_2 = decode_layer2(decoding_1)\n",
    "reconstruction = recon_layer(decoding_2)\n",
    "\n",
    "ae = Model(inputs=inputs, outputs=reconstruction)\n",
    "#monitor = EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "DEBUG = False\n",
    "\n",
    "class WeightHistory(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        print(batch)\n",
    "        print('coding layer weights')\n",
    "        print(self.model.get_layer('coding_layer').get_weights())\n",
    "        print(self.model.get_layer('coding_layer').get_weights()[0].shape)\n",
    "        print('decoding layer weights')\n",
    "        print(np.transpose(self.model.get_layer('decode_layer1').get_weights()))\n",
    "        print(np.transpose(self.model.get_layer('decode_layer1').get_weights()[0]).shape)\n",
    "\n",
    "\n",
    "history = WeightHistory()\n",
    "\n",
    "if DEBUG:\n",
    "    callbacks.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8284/8284 [==============================] - 173s 21ms/step - loss: 9.3084e-06\n",
      "Epoch 2/2\n",
      "8284/8284 [==============================] - 169s 20ms/step - loss: 9.2530e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3281ecd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.fit(x_train, x_train, epochs=2, verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding_model = Model(inputs=ae.inputs, outputs=ae.get_layer(\"the_code\").output)\n",
    "coding_model = Model(inputs=inputs, outputs=the_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save the embeddings in order to plot them later\n",
    "# serialize model to JSON\n",
    "model_json = coding_model.to_json()\n",
    "with open(\"coding_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "coding_model.save_weights(\"coding_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
